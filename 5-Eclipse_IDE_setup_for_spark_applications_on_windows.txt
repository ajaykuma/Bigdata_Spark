Install eclipse
Make sure you have scala plugin installed in eclipse
(say : http://download.scala-ide.org/sdk/lithium/e47/scala212/stable/site)

or download scale IDE - eclipse
http://scala-ide.org/download/sdk.html

Make sure scala compiler is set to scala-2.11  (or scala-2.12 as per your IDE)
(project > build path > configure build path > scala compiler)

Download spark package for hadoop which contains
spark jars in <spark path/jars>

Add sparks jars so that your code can compile (project > build path > configure build path > java build path > 
libraries > add external jar )
Note** This is needed when we intend to test and run applications on local spark.
If build apps into jar using SBT, we can avoid this step.

Make sure your conf points to master as local within your code
OR
go to run confiurations> and under environment tab click on new and add
SPARK_LOCAL_IP
127.0.0.1

#(If hadoop not downloaded and setup, then create a folder "hadoop" and "bin" within "hadoop" 
HADOOP_HOME=C:\hadoop\

#Look into: https://github.com/cdarlint/winutils
--Download 'hadoop.dll' & 'winutils.exe' as per your downloaded version of spark
--copy these into C:/hadoop/bin

go to run confiurations > and under environment tab click on new and add
HADOOP_HOME = C:\hadoop

Note** It should be like this
C:\hadoop\bin\winutils.exe

Write your code(scala/spark)
Run your application..

Note** build.sbt can be added to your project, in case you intend to use SBT to package code as JAR to run on a 
       hadoop cluster using spark-submit.
       In this case download SBT on your machine, add build.sbt <provided in link> to your project, 
       write your code [for your code to compile you can add spark
       jars to the build path and set your scala compiler to scala-2.11 build.]
       From cmd line, go into your project directory, give a command 'sbt package' which packages your code into jar 
       and then this jar can be moved to cluster.
       For running spark application on cluster refer - "Running Spark Applications on Cluster"
       
If not using SBT, then we dont need sbt or build.sbt setup. Just spark related jars in build path, 
code in eclipse and environment variables set .



