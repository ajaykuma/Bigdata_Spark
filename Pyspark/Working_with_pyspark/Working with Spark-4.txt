Recap:
SparkContext-sc--->RDD
SparkSession-spark--->SchemaRDD (dataframe)
(pyspark/spark-shell)

RDD:
Operations: transformations , actions
Partitioning
Chaining of transformations
Pipelining
caching n different options (cache()/persist()/from pyspark import StorageLevel---using different options to persist

sc > textFile/wholeTextFiles/parallelize
sc > map/flatMap/filter/groupBy/reduceBy/sortBy/groupByKey/reduceByKey/countBykey
     zip/sub/union/join (if ur rdd is a pair rdd (k,v)
spark > read/write(default=parquet), format/json, read-df/ds,transformations,save
repartition()
toDebugString()

--------------------

Pair RDD: data > (K,V) Ex: This is my content---> This, This is my content
groupBy
reduceBY
groupByKey
reduceByKey
countByKey

PairRDD
zip/sub/union/join/...
------------

>>> x = sc.textFile("mydata/mydata-local1/abc1.txt")
>>> y = sc.textFile("mydata/mydata-local1/abc2.txt")

>>> pairrdd = x.map(lambda n: n.split(" ")).map(lambda n: (n[0],n))
>>> pairrdd2 = y.map(lambda n: n.split(" ")).map(lambda n: (n[0],n))
>>> union_data = pairrdd.union(pairrdd2)
>>> union_data.collect()
>>> for i in union_data.take(50):
...     print(i)
>>> union_data.getNumPartitions()
4
>>> pairrdd.getNumPartitions()
2
>>> pairrdd2.getNumPartitions()
2

pairrdd.subtract(pairrdd2)
pairrdd.intersection(pairrdd2)
pairrdd.zip(pairrdd2)
pairrdd.join(pairrdd2)
=================
option 1.1: defining schema and reading json file

from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType,StringType,StructType,StructField
spark = SparkSession.builder.appName("testdfapp").getOrCreate()

myschema = StructType([StructField("id",IntegerType(),True),
                       StructField("name",StringType(),True),
                       StructField("age",IntegerType(),True),
                       StructField("city",StringType(),True),
                       StructField("address",StringType(),True)])
people = spark.read.schema(myschema).json("mydata/mydata-local1/people.json")
"""recipes = spark.read.option("multiLine","true").json("mydata/mydata-local1/recipes2.json")"""
people.show()

Option 1.2
from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType,StringType,StructType,StructField
spark = SparkSession.builder.appName("testdfapp").config("spark.sql.warehouse.dir","/user/itv/spark-warehouse").getOrCreate()

myschema = StructType([StructField("id",IntegerType(),True),
                       StructField("name",StringType(),True),
                       StructField("age",IntegerType(),True),
                       StructField("city",StringType(),True),
                       StructField("address",StringType(),True)])
people = spark.read.schema(myschema).json("mydata/mydata-local1/people.json")
people.show()
people.write.saveAsTable("peopledf1")

option 2: directly reading json file
>>> df = spark.read.json("mydata/mydata-local1/people.json")
>>> df.show()

>>> df.printSchema()

>>> df.select("age")

>>> df.select("age","address")

>>> df.select(df['age'])

>>> df.select(df['age'] > 30)

>>> df.filter(df['age'] > 30)

>>> df.groupBy("age").count()

>>>df.groupBy("age").count().write.saveAsTable("ajmydb.dbmyfirstdf")

>>> df.groupBy("age").count().write.mode("overwrite").saveAsTable("ajmydb.dbmyfirstdf")

>>>spark.sql("use ajmydb")
>>>spark.sql("show tables")
>>>spark.sql("select * from dbmyfirstdf").show()

#writes data in parquet
>>> df.groupBy("age").count().write.mode("overwrite").save("dbmyfirstdf")

#write the data in json
>>> df.groupBy("age").count().write.format("json").mode("overwrite").save("dbmyfirstdf")

>>> df.groupBy("age").count().write.format("csv").mode("overwrite").save("dbmyfirstdf1")
>>> df.groupBy("age").count().write.format("csv").mode("append").save("dbmyfirstdf1")

>>> df.createOrReplaceTempView("people")
>>> spark.sql("select * from people")

>>> df.createGlobalTempView("people1")
>>> spark.sql("select * from global_temp.people1").show()
>>> spark.newSession().sql("select * from global_temp.people1").show()

Bank_full.csv
>>> df = spark.read.format("csv").option("inferSchema","true").option("header","true").load("mydata/mydata-local1/Bank_full.csv")
>>> df.show(5)
>>> df.columns
>>> df[df.age > 80].show()
>>> df[df.age > 80].count()
99
>>>df.summary().show()

write the df in different formats
>>> df = spark.read.format("csv").option("inferSchema","true").option("header","true").load("mydata/mydata-local1/Bank_full.csv")
>>> df.rdd.getNumPartitions() 
>>> df.write.format("json").mode("overwrite").save("dbmyfirstdf2")
>>> df2 = spark.read.json("dbmyfirstdf2/part-00000-0b8ce26e-ddfe-4388-a16a-d652edd66d97-c000.json")                                     
>>> df2.show(2)

>>> df.filter(df['balance'] > 2000).count()
8501
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married')).count()
5375
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')).count()
1690

>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')).show(10)
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')).sort('balance',ascending=False).show(10)

>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')) \
... .groupBy('job').count()

>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')) \
... .groupBy('job').count().sort('count',ascending=False).show(5)

>>> selecteddf = df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')) \
... .groupBy('job').count().sort('count',ascending=False)

>>> df.count()
45211

>>> selecteddf.count()
12

df.groupBy(df.job).count()

>>> from pyspark.sql.functions import countDistinct
>>> df.select(countDistinct('job'))

>>> df[(df.age.isin(['35','25','45','55']))].show(10)

df.groupBy(['age','y']).count()

--task(sort the above group by result in descending order of count and find top 10 counts)

df.groupBy(['marital','age','y']).count()

df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (df.marital == 'married')).show()

from pyspark.sql.functions import avg,collect_list
df.select(collect_list("job")).show()
df.select(avg("balance"))

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (df.marital == 'married')).\
... write.option("header","true").\
... mode("overwrite").\
... csv("myfirstdf4")

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (df.marital == 'married')).rdd.getNumPartitions()
200
>>> df.rdd.getNumPartitions()
1

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (df.marital == 'married')).\
... write.option("header","true").\
... partitionBy("marital").\
... mode("overwrite").\
... csv("myfirstdf5")

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (df.marital == 'married')).\
... write.option("header","true").\
... partitionBy("age").\
... mode("overwrite").\
... csv("myfirstdf5")

>>> df.\                                               
... write.option("header","true").\
... partitionBy("job").\
... mode("overwrite").\
... csv("myfirstdf6")

>>> newdf = spark.read.option("inferSchema","true").option("header","true").csv("myfirstdf6/job=admin.")

>>> newdf.columns
['serNo', 'age', 'marital', 'education', 'defaulter', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y']
>>> df.columns
['serNo', 'age', 'job', 'marital', 'education', 'defaulter', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y']                                                                                                  
>>> newdf.show(10)

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (df.marital == 'married')).\
... repartition(1).\
... write.option("header","true").\
... partitionBy("marital").\
... mode("overwrite").\
... csv("myfirstdf7")

>>> from pyspark.sql.functions import udf
>>> from pyspark.sql.functions import UserDefinedFunction

create your function:
>>> def agegroup(x):
...     if x>50:
...             return 'old'
...     else:
...             return 'young'
... 

>>> from pyspark.sql.types import StringType
>>>udf = UserDefinedFunction(lambda x: agegroup(x), StringType())
>>>newdf = df.withColumn('age',udf(df.age))
>>> df = df.withColumn('ageGroup',udf(df.age))

-------------------------------------------













write the df with overwrite n append options
write the df in a hive table

================








