#Set 4
#DataFrame

Ex1:
--using file from hdfs and using SparkSession which is available as 'spark' ie dataframe api (schemaRDD)
>>> x = spark.read.load("mydata/mydata-local1/Bank_full.csv")
--will fail as data is not in parquet

>>> x = spark.read.load("mydata/mydata-local1/users.parquet")
>>> x.first()

>>> x = spark.read.format("csv").load("mydata/mydata-local1/Bank_full.csv")

>>> type(x)
<class 'pyspark.sql.dataframe.DataFrame'>

>>> x.printSchema()
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)
 |-- _c12: string (nullable = true)
 |-- _c13: string (nullable = true)
 |-- _c14: string (nullable = true)
 |-- _c15: string (nullable = true)
 |-- _c16: string (nullable = true)
 |-- _c17: string (nullable = true)

>>> x = spark.read.format("csv").option("header","true").option("delimiter",",").option("inferSchema","true").load("mydata/mydata-local1/Bank_full.csv")

>>> type(x)                                                                                                                                          
<class 'pyspark.sql.dataframe.DataFrame'>

>>> x.printSchema()root
 |-- serNo: integer (nullable = true)
 |-- age: integer (nullable = true)
 |-- job: string (nullable = true)
 |-- marital: string (nullable = true)
 |-- education: string (nullable = true)
 |-- defaulter: string (nullable = true)
 |-- balance: integer (nullable = true)
 |-- housing: string (nullable = true)
 |-- loan: string (nullable = true)
 |-- contact: string (nullable = true)
 |-- day: integer (nullable = true)
 |-- month: string (nullable = true)
 |-- duration: integer (nullable = true)
 |-- campaign: integer (nullable = true)
 |-- pdays: integer (nullable = true)
 |-- previous: integer (nullable = true)
 |-- poutcome: string (nullable = true)
 |-- y: string (nullable = true)

>>>x.filter(x.marital == 'married')

>>>x.filter(x.marital == 'married').count()

Ex2:
Option1:
option 2: directly reading json file
>>> df = spark.read.json("mydata/mydata-local1/people.json")

>>> df.show()

>>> df.printSchema()

>>> df.select("age")

>>> df.select("age","address")

>>> df.select(df['age'])

>>> df.select(df['age'] > 30)

>>> df.filter(df['age'] > 30)

>>> df.groupBy("age").count()

--if database ajmydb exists
>>>df.groupBy("age").count().write.saveAsTable("ajmydb.dbmyfirstdf")
or
>>> df.groupBy("age").count().write.mode("overwrite").saveAsTable("ajmydb.dbmyfirstdf")

--if not
df.groupBy("age").count().write.saveAsTable("dbmyfirstdf")

--using hive if database exists
>>>spark.sql("use ajmydb")

or
>>> x = spark.sql("select * from dbmyfirstdf limit 10")
>>> type(x)
<class 'pyspark.sql.dataframe.DataFrame'>
x = spark.sql("select * from dbmyfirstdf limit 10").show(2)

Option2:
option 1.1: defining schema and reading json file

from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType,StringType,StructType,StructField
spark = SparkSession.builder.appName("testdfapp").getOrCreate()

myschema = StructType([StructField("id",IntegerType(),True),
                       StructField("name",StringType(),True),
                       StructField("age",IntegerType(),True),
                       StructField("city",StringType(),True),
                       StructField("address",StringType(),True)])
people = spark.read.schema(myschema).json("mydata/mydata-local1/people.json")
people.show()

[--if using a multiline json file
"""recipes = spark.read.option("multiLine","true").json("mydata/mydata-local1/recipes2.json")"""]

Option 1.2
from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType,StringType,StructType,StructField
spark = SparkSession.builder.appName("testdfapp").config("spark.sql.warehouse.dir","/user/win10/spark-warehouse").getOrCreate()

myschema = StructType([StructField("id",IntegerType(),True),
                       StructField("name",StringType(),True),
                       StructField("age",IntegerType(),True),
                       StructField("city",StringType(),True),
                       StructField("address",StringType(),True)])
people = spark.read.schema(myschema).json("mydata/mydata-local1/people.json")
people.show()
people.write.saveAsTable("peopledf1")


--if using Hive/if database exists
>>>spark.sql("use ajmydb")

--if not then directly
>>>spark.sql("show tables")
>>>spark.sql("select * from dbmyfirstdf").show()

#writes data in parquet
>>> df = spark.read.json("mydata/mydata-local1/people.json")
>>> df.groupBy("age").count().write.mode("overwrite").save("dbmyfirstdf")

#write the data in json
>>> df.groupBy("age").count().write.format("json").mode("overwrite").save("dbmyfirstdf")
>>> df.groupBy("age").count().write.format("csv").mode("overwrite").save("dbmyfirstdf1")
>>> df.groupBy("age").count().write.format("csv").mode("append").save("dbmyfirstdf1")

>>> df.createOrReplaceTempView("people")
>>> spark.sql("select * from people")

>>> df.createGlobalTempView("people1")
>>> spark.sql("select * from global_temp.people1").show()
>>> spark.newSession().sql("select * from global_temp.people1").show()

Ex3:
>>> df = spark.read.format("csv").option("inferSchema","true").option("header","true").load("mydata/mydata-local1/Bank_full.csv")
>>> df.show(5)
>>> df.columns
>>> df[df.age > 80].show()
>>> df[df.age > 80].count()
>>>df.summary().show()

write the df in different formats
>>> df.rdd.getNumPartitions() 
>>> df.write.format("json").mode("overwrite").save("dbmyfirstdf2")
>>> df2 = spark.read.json("dbmyfirstdf2/part-00000-0b8ce26e-ddfe-4388-a16a-d652edd66d97-c000.json")                                     
>>> df2.show(2)

>>> df.filter(df['balance'] > 2000).count()
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married')).count()
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')).count()
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')).show(10)
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')).sort('balance',ascending=False).show(10)
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')) \
... .groupBy('job').count()
>>> df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')) \
... .groupBy('job').count().sort('count',ascending=False).show(5)
>>> selecteddf = df.filter((df['balance'] > 2000) & (df.marital == 'married') & (df.education == 'tertiary')) \
... .groupBy('job').count().sort('count',ascending=False)
>>> df.count()
>>> selecteddf.count()
>>>df.groupBy(df.job).count()

>>> from pyspark.sql.functions import countDistinct
>>> df.select(countDistinct('job'))
>>> df[(df.age.isin(['35','25','45','55']))].show(10)
>>>df.groupBy(['age','y']).count()

--adding option for compress
>>> df[(df.age.isin(['35','25','45','55']))].write.format("csv").option("codec","org.apache.hadoop.io.compress.GzipCodec").mode("overwrite").save("/sampledata/data_in_csv")

--task(sort the above group by result in descending order of count and find top 10 counts)

>>>df.groupBy(['marital','age','y']).count()
>>>df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (col("marital") == 'married')).show()

>>>from pyspark.sql.functions import avg,collect_list
>>>df.select(collect_list("job")).show()
>>>df.select(avg("balance"))

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (col("marital") == 'married')).\
... write.option("header","true").\
... mode("overwrite").\
... csv("myfirstdf4")

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (col("marital") == 'married')).rdd.getNumPartitions()
>>> df.rdd.getNumPartitions()

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (col("marital") == 'married')).\
... write.option("header","true").\
... partitionBy("marital").\
... mode("overwrite").\
... csv("myfirstdf5")

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (col("marital") == 'married')).\
... write.option("header","true").\
... partitionBy("age").\
... mode("overwrite").\
... csv("myfirstdf5")

>>> df.\                                               
... write.option("header","true").\
... partitionBy("job").\
... mode("overwrite").\
... csv("myfirstdf6")

>>> newdf = spark.read.option("inferSchema","true").option("header","true").csv("myfirstdf6/job=admin.")

>>> newdf.columns

>>> df.columns

>>> newdf.show(10)

>>> df.groupBy(['marital','age','y']).count().filter((col("count") > 500) & (col("marital") == 'married')).\
... repartition(1).\
... write.option("header","true").\
... partitionBy("marital").\
... mode("overwrite").\
... csv("myfirstdf7")

>>> from pyspark.sql.functions import udf
>>> from pyspark.sql.functions import UserDefinedFunction

create your function:
>>> def agegroup(x):
...     if x>50:
...             return 'old'
...     else:
...             return 'young'
... 

>>> from pyspark.sql.types import StringType
>>>udf = UserDefinedFunction(lambda x: agegroup(x), StringType())
>>>newdf = df.withColumn('age',udf(df.age))
>>> df = df.withColumn('ageGroup',udf(df.age))

-------------------------------------------
write the df with overwrite n append options
write the df in a hive table

================
