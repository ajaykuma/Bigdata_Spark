#Set 2

x = sc.textFile("mydata/mydata-local1/abc1.txt")
y = x.flatMap(lambda n: n.split(" "))
numTs = y.filter(lambda n: 't' in n)
numWs = y.filter(lambda n: 'w' in n)
numTs.count()
numWs.count()
print("number of ts in file:" ,numTs.count(),"number of ws in file: " ,numWs.count())

------------
Application

DAG /Job 0/ Stage 0
x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
y = x.flatMap(lambda n: n.split(" "))
z = y.map(lambda word: (word,1))
z.first()

DAG Job 1/Stage 1
# x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
# y = x.flatMap(lambda n: n.split(" "))
# z = y.map(lambda word: (word,1))
z.collect()

DAG  Job 2/Stage 2
# x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
# y = x.flatMap(lambda n: n.split(" "))
# z = y.map(lambda word: (word,1))
z.saveAsTextFile("mydata/rddo2")

DAG Job 3/Stage 3 ( involves shuffling)
# x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
# y = x.flatMap(lambda n: n.split(" "))
# z = y.map(lambda word: (word,1))
z.repartition(1).saveAsTextFile("mydata/rddo3")

DAG Job 4/Stage 4 ( involves shuffling)
# x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
# y = x.flatMap(lambda n: n.split(" "))
# z = y.map(lambda word: (word,1))
wordcount = z.reduceByKey(lambda a,b: a+b)
wordcount.saveAsTextFile("mydata/rddo4")

DAG Job 5/Stage 5 ( involves shuffling)
# x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
# y = x.flatMap(lambda n: n.split(" "))
# z = y.map(lambda word: (word,1))
#wordcount = z.reduceByKey(lambda a,b: a+b)
wordcount.repartition(1).saveAsTextFile("mydata/rddo5")

--Pair RDD: data > (K,V) Ex: This is my content---> This, This is my content

--When working with directory
>>>mydata = sc.wholeTextFiles("mydata/mydata-local1")
>>> for i in mydata.keys().take(10):
...     print(i)

--caching
x = sc.textFile("mydata/mydata-local1/abc1.txt",4).flatMap(lambda n: n.split(" ")).map(lambda word: (word,1)).cache()
x.collect()

--persisting
y = sc.textFile("mydata/mydata-local1/abc1.txt",4).flatMap(lambda n: n.split(" ")).map(lambda word: (word,1)).persist()
y.collect()

Resource mgr > application > applicationMaster

--Persisting Options
StorageLevel.
StorageLevel.MEMORY_ONLY_SER ---SAME AS memory_only , stores RDD as serialized objects in JVM memory
StorageLevel.MEMORY_ONLY_2 ---replicated cached partitions of RDD
StorageLevel.MEMORY_ONLY_SER_2 
StorageLevel.MEMORY_AND_DISK
StorageLevel.MEMORY_ONLY
StorageLevel.MEMORY_AND_DISK_2
StorageLevel.MEMORY_AND_DISK_ONLY
StorageLevel.DISK_ONLY
StorageLevel.DISK_ONLY_2

>>> from pyspark import StorageLevel
>>> x = sc.textFile("mydata/mydata-local1/abc1.txt",4)
>>> y = x.flatMap(lambda n: n.split(" "))
>>> z = y.map(lambda word: (word,1))

>>> z.persist(StorageLevel.DISK_ONLY)
result is 'PythonRDD - Disk serialized 1x Replicated'

>>> z.unpersist()

>>> z.persist(StorageLevel.MEMORY_ONLY)
result is 'PythonRDD - Memory serialized 1x Replicated'

>>> z.persist(StorageLevel.MEMORY_AND_DISK_ONLY)

>>> z.collect()

>>> print(z.getStorageLevel())

===================================
