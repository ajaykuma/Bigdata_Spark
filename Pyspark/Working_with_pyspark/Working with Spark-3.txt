
PyCharm:

testapp1.py
from pyspark import SparkConf, SparkContext

sc = SparkContext(master="local",appName="testAppl")
mydata = sc.textFile("file:///D:\\GitContent\\Datasets\\Datasets\\cv000_29416.txt",2)
step1 = mydata.map(lambda n: n.upper())
print(step1.collect())
print("number of partitions:", mydata.getNumPartitions())

spark-submit --master local --deploy-mode client --driver-memory 1g --executor-memory 500m --executor-cores 1 testapp1.py

testapp2.py
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local").appName("sparkdfex").getOrCreate()

x = spark.read.format("json").option("inferSchema","true").option("header","true").\
        load("file:///D:\\GitContent\\Datasets\\Datasets\\employees.json")
x.show(10)
x.printSchema()

spark-submit --master local --deploy-mode client --driver-memory 1g --executor-memory 500m --executor-cores 1 testapp2.py
----------------

ex1:
>>> x = "welcome"
>>> y = sc.parallelize(x)
>>> y.first()
'w'                                                                                                                                                   
>>> z = y.map(lambda n: n.upper())

>>> z.first()
'W'

>>> t = z.filter(lambda n: n == 'W')
>>> t.count()
1                                                                                                                                                     
>>> t = z.filter(lambda n: n == 'E')
>>> t.count()
2

ex2:

>>> x = range(1,10)
>>> y = sc.parallelize(x)
>>> y
ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262
>>> z = y.map(lambda n : n*100)
>>> z
PythonRDD[1] at RDD at PythonRDD.scala:53
>>> z.collect()
[100, 200, 300, 400, 500, 600, 700, 800, 900]                                                                                                         
>>> x = sc.textFile("mydata/mydata-local1/abc1.txt")
>>> x
mydata/mydata-local1/abc1.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0

ex3:
x = sc.textFile("mydata/mydata-local1/abc1.txt")
>>> y = x.map(lambda n: (n.split(" "))).map(lambda n: (n[0],n))
>>>grouped = y.groupByKey().mapValues(list)
>>>grouped.collect()

>>> count = y.groupByKey().count()

ex4:
option1:
>>> x = sc.textFile("mydata/mydata-local1/abc1.txt")
>>> pairrdd = x.map(lambda n: n.split(" ")).map(lambda n: (n[0],n))

>>> pairrdd.first()
(u'This', [u'This', u'is', u'my', u'sample', u'file'])                                                                                  

>>> for i in pairrdd.keys().take(10):
...     print(i)

>>> for i in pairrdd.values().take(10):
...     print(i)

>>> y = sc.textFile("mydata/mydata-local1/abc2.txt")
>>> pairrdd2 = y.map(lambda n: n.split(" ")).map(lambda n: (n[0],n))

>>> for i in pairrdd.take(20):
...     print(i)
>>> for i in pairrdd2.take(20):
...     print(i)


>>> for i in pairrdd.sortByKey().take(20):
...     print(i)
>>> for i in pairrdd2.sortByKey().take(20):
...     print(i)

option2:
x = sc.textFile("mydata/mydata-local1/abc1.txt")
result = x.map(lambda n: n.split(" ")).sortBy(lambda n: n[0])
>>> for i in pairrdd.sortBy(lambda n: n[0]).take(20):
...     print(i)

option3:
>>> for i in pairrdd.sortByKey(True, 1).take(5):
...     print(i)

>>> result = pairrdd.sortByKey(ascending=True).values()
>>> for i in result.take(20):
...     print(i)
>>> result1 = pairrdd.sortByKey(ascending=False).values()
>>> for i in result1.take(20):
...     print(i)

>>>pairrdd.countByKey().items()

>>> reduced = pairrdd.reduceByKey(lambda a,b: a+b)
>>> for i in reduced.take(10):
...     print(i)

--------------------

Q: given a json file
   --overwrite to hdfs
     in spark using python>
   --create a df from provided file
   --write code to run a select on df
   --register this df as a temp table 
   --write a simple select query
   --save this as a new df > save this to hdfs
   --save the resultant df as a table




