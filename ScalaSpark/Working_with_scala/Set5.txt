
#Spark broadcast variables: immutable shared variables which are cached on each worker node on a cluster
When to use Broadcast variable?
Before running each tasks on the available executors, Spark computes the task’s closure. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD.
If you have huge array that is accessed from Spark Closures, for example some reference data, this array will be shipped to each spark node with closure.
For example if you have 10 nodes cluster with 100 partitions (10 partitions per node), this Array will be distributed at least 100 times (10 times to each node).
If you use broadcast it will be distributed once per node using efficient p2p protocol.
val array: Array[Int] = ??? // some huge array
val broadcasted = sc.broadcast(array)
And some RDD
val rdd: RDD[Int] = ???
In this case array will be shipped with closure each time
rdd.map(i => array.contains(i))
and with broadcast you'll get huge performance benefit
rdd.map(i => broadcasted.value.contains(i))

Things to remember while using Broadcast variables:
Once we broadcasted the value to the nodes, we shouldn’t make changes to its value to make sure each node have exact same copy of data. The modified value might be sent to another node later that would give unexpected results.

#1.Joining a large and a small RDD
If the small RDD is small enough to fit into the memory of each worker we can turn it into a broadcast variable and turn the entire operation into a so called map side join for the larger RDD
In this way the larger RDD does not need to be shuffled at all. This can easily happen if the smaller RDD is a dimension table.

val smallRDD = paira
val largeRDD = pairb

val smallLookup = sc.broadcast(smallRDD.collect.toMap)
val resu = largeRDD.flatMap { case(key, value) =>
  smallLookup.value.get(key).map { otherValue =>
    (key, (value, otherValue))
  }
}

resu.collect()

#2.Joining a large and a medium size RDD
If the medium size RDD does not fit fully into memory but its key set does, it is possible to exploit this . As a join will discard all elements of the larger RDD that do not have a matching partner in the medium size RDD, we can use the medium key set to do this before the shuffle. If there is a significant amount of entries that gets discarded this way, the resulting shuffle will need to transfer a lot less data.

val mediumRDD = paira
val keys = sc.broadcast(mediumRDD.map(_._1).collect.toSet)
val reducedRDD = largeRDD.filter{ case(key, value) => keys.value.contains(key) }
reducedRDD.join(mediumRDD).collect()

It is important to note that the efficiency gain here depends on the filter operation actually reducing the size of the larger RDD.

=====================================
