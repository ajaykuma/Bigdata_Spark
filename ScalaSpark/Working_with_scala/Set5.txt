--to be customized for scala

#Set 5
x = spark.read.format("csv").option("header","true").option("inferSchema","true").load("I:\\GitContent\\Datasets\\Datasets\\Bank_full.csv")
x.columns
df[df.age > 80]
df.createOrReplaceTempView("test")
spark.sql("select * from test where age > 80" ).show()
df[df.age > 80].count()
df.summary().show()
df[(df.age > 80) & (df.marital=='married')],show()
df[(df.age > 80) & (df.marital=='married')].sort('balance',ascending=False).show(5)

df[(df.age.isin(['82','15','17','35']))].show(5)

df.groupby(['age','y']).count()

#If we want to use different fields for sorting, or DESC instead of ASC
# we want to sort by our calculated field (count), this field needs to become part of 
# the DataFrame. After grouping , we get back a different type, called 
# a GroupByObject. 
# So we need to convert it back to a DataFrame. 
x = df.groupby(['marital', 'y']).count()
x.sort('marital','count',ascending=False).show()

#additionally filter grouped data using a HAVING condition. In Pandas, 
# you can use .filter() and provide a Python function (or a lambda) 
# that will return True if the group should be included into the result.

y = df[df.marital == 'married'].groupby('y').count()
y.show()
from pyspark.sql.functions import col
y.filter(col("count")>3000).show()

#creating new df
df1 = df.groupby(['age','y']).count()

>>> from pyspark.sql import Row
>>> file = sc.textFile("mydata/mydata-local1/abc1.txt")
>>> partfile = file.map(lambda n: n.split(" "))
>>> partfile.collect()
>>> filecont = partfile.map(lambda n: Row(keyword=n[0],word1=n[1],word2=n[2],word3=n[3]))
>>> schemafilecontdf = spark.createDataFrame(filecont)
>>> schemafilecontdf.show()
>>> schemafilecontdf.createOrReplaceTempView("fileindf")
>>> spark.sql("select * from fileindf")
-------------------
>>> file = spark.read.text("mydata/mydata-local1/abc1.txt")
>>> file.show(truncate=False)

-------------------
>>> file = sc.textFile("mydata/mydata-local1/people.txt")
>>> partfile = file.map(lambda n: n.split(","))
>>> filecont = partfile.map(lambda n: (n[0],n[1])
... )
>>> for i in filecont.take(10):
...     print(i)
>>> schemat = "name age"
>>> from pyspark.sql.types import *
>>> fields = [StructField(field_name, StringType(), True) for field_name in schemat.split()]
>>> schema = StructType(fields)
>>> df = spark.createDataFrame(filecont,schema)
>>> df.show()
======================
