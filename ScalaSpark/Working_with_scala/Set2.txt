#Set 2

val x = sc.textFile("/sampledata/abc1.txt")
val y = x.flatMap(n => n.split(" "))
val numAs = y.filter(line => line.contains("a")).count() 
val numBs = y.filter(line => line.contains("b")).count() 
numAs
numBs

------------
Understanding Application
--depending on setup we read and save output in HDFS or local location

DAG /Job 0/ Stage 0
val x = sc.textFile("/sampledata/abc1.txt")
val y = x.flatMap(n => n.split(" "))
val z = y.map(n => (n,1))
z.first()

DAG Job 1/Stage 1
val x = sc.textFile("/sampledata/abc1.txt")
val y = x.flatMap(n => n.split(" "))
val z = y.map(n => (n,1))
z.collect()

DAG  Job 2/Stage 2
val x = sc.textFile("/sampledata/abc1.txt")
val y = x.flatMap(n => n.split(" "))
val z = y.map(n => (n,1))
z.saveAsTextFile("mydata/rddo2")

DAG Job 3/Stage 3 ( involves shuffling)
val x = sc.textFile("/sampledata/abc1.txt")
val y = x.flatMap(n => n.split(" "))
val z = y.map(n => (n,1))
z.repartition(1).saveAsTextFile("mydata/rddo3")

DAG Job 4/Stage 4 ( involves shuffling)
val x = sc.textFile("/sampledata/abc1.txt")
val y = x.flatMap(n => n.split(" "))
val z = y.map(n => (n,1))
val wordcount = z.reduceByKey(_+_)
wordcount.saveAsTextFile("mydata/rddo4")

DAG Job 5/Stage 5 ( involves shuffling)
wordcount.repartition(1).saveAsTextFile("mydata/rddo5")

--Pair RDD: data > (K,V) Ex: This is my content---> This, This is my content

--When working with directory
val mydata = sc.wholeTextFiles("/sampledata")

or

val x = sc.wholeTextFiles("file:////usr/local/spark/examples/src/main/resources")
x: org.apache.spark.rdd.RDD[(String, String)] = file:////usr/local/spark/examples/src/main/resources MapPartitionsRDD[1] at wholeTextFiles at <console>:24

x.getNumPartitions
res0: Int = 1

x.keys
res1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at keys at <console>:26

x.keys.take(10).foreach(println)
file:/usr/local/spark/examples/src/main/resources/people.json                   
file:/usr/local/spark/examples/src/main/resources/user.avsc
file:/usr/local/spark/examples/src/main/resources/users.avro
file:/usr/local/spark/examples/src/main/resources/kv1.txt
file:/usr/local/spark/examples/src/main/resources/users.orc
file:/usr/local/spark/examples/src/main/resources/users.parquet
file:/usr/local/spark/examples/src/main/resources/people.csv
file:/usr/local/spark/examples/src/main/resources/employees.json
file:/usr/local/spark/examples/src/main/resources/full_user.avsc
file:/usr/local/spark/examples/src/main/resources/people.txt

x.values.take(10).foreach(println)

--searching for a particular key
x.keys.filter(n => n.contains("people")).take(5).foreach(println)
file:/usr/local/spark/examples/src/main/resources/people.json
file:/usr/local/spark/examples/src/main/resources/people.csv
file:/usr/local/spark/examples/src/main/resources/people.txt

--looking at key and value
x.take(1).foreach(println)
(file:/usr/local/spark/examples/src/main/resources/people.json,{"name":"Michael"}
{"name":"Andy", "age":30}
{"name":"Justin", "age":19}
)

--caching
val x = sc.textFile("/sampledata/abc1.txt",4).flatMap(n => n.split(" ")).map(word => (word,1)).cache()
x.collect()

--persisting
val y = sc.textFile("/sampledata/abc1.txt",4).flatMap(n => n.split(" ")).map(word => (word,1)).persist()
y.collect()

Resource mgr > application > applicationMaster

--Persisting Options
StorageLevel.
StorageLevel.MEMORY_ONLY_SER ---SAME AS memory_only , stores RDD as serialized objects in JVM memory
StorageLevel.MEMORY_ONLY_2 ---replicated cached partitions of RDD
StorageLevel.MEMORY_ONLY_SER_2 
StorageLevel.MEMORY_AND_DISK
StorageLevel.MEMORY_ONLY
StorageLevel.MEMORY_AND_DISK_2
StorageLevel.MEMORY_AND_DISK_ONLY
StorageLevel.DISK_ONLY
StorageLevel.DISK_ONLY_2

>>> from pyspark import StorageLevel
>>> val x = sc.textFile("/sampledata/abc1.txt",4)
>>> val y = x.flatMap(n => n.split(" "))
>>> val z = y.map(word => (word,1))
>>> z.persist(StorageLevel.DISK_ONLY)
>>> z.unpersist()
>>> z.persist(StorageLevel.MEMORY_ONLY)
>>> z.persist(StorageLevel.MEMORY_AND_DISK_ONLY)
>>> z.collect()

===================================
