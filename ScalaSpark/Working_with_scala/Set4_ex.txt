#Set 4
#DataFrame

#creating dataframes

scala> import spark.implicits._
import spark.implicits._

scala> val columns = Seq("ProgLang","developers")

scala> columns

scala> val data = Seq(("Go","5000"),("Python","1000"),("Scala","10000"))

scala> data

scala> val data1 = sc.parallelize(data)

scala> val df = data1.toDF()
df: org.apache.spark.sql.DataFrame = [_1: string, _2: string]

scala> df.printSchema
root
 |-- _1: string (nullable = true)
 |-- _2: string (nullable = true)


scala> val df = data1.toDF("ProgLang","developers")
df: org.apache.spark.sql.DataFrame = [ProgLang: string, developers: string]

scala> df.show()

#using createDataFrame
scala> val df2 = spark.createDataFrame(data1).toDF(columns:_*)
df2: org.apache.spark.sql.DataFrame = [ProgLang: string, developers: string]

scala> df2.show()

#using Row RDD and schema
scala> import org.apache.spark.sql.types.{StringType, StructField, StructType}
import org.apache.spark.sql.types.{StringType, StructField, StructType}

scala> import org.apache.spark.sql.Row
import org.apache.spark.sql.Row

scala> val schema = StructType (Array (
     |                StructField("ProgLang", StringType, true),
     |                StructField("developers", StringType, true)))
schema: org.apache.spark.sql.types.StructType = StructType(StructField(ProgLang,StringType,true), StructField(developers,StringType,true))

scala> val rowRDD = data1.map(attr => Row(attr._1, attr._2))
rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[9] at map at <console>:30

scala> val df3 = spark.createData
createDataFrame   createDataset

scala> val df3 = spark.createDataFrame(rowRDD, schema)
df3: org.apache.spark.sql.DataFrame = [ProgLang: string, developers: string]

scala> df3.show()

----------------
#working with file 'After_Markings_SMPC522A1DC0040_SE-PGA454_20171012_082108_FileInfo.txt'
--refer the metadata on top 
category,startRow,nRows,type
environmentMetaData,13,2,emd
fileMetaData,15,2,fmd
systemMetaData,17,2,smd
vehicleInformationMetaData,19,2,vimd
visionMetaData,21,2,vmd
Pedestrian,23,219,Pedestrian
TwoWheeler,242,925,TwoWheeler
Unclassified3DMarking,1167,2721,Unclassified3DMarking
Vehicle,3888,2474,Vehicle

--let's create dataframe by extracting 'pedestrian' data ie 
columns from row number 23
data is 218 rows from row #24

#specifying columns
scala> val columns = "frameNumber,streamName,refId,objectType,height,direction,movement,occlusion,headOccluded,feetOccluded,overlapped,unsharp,strangePose,crossing,accessory,topLeftX,topLeftY,topRightX,topRightY,bottomRightX,bottomRightY,bottomLeftX,bottomLeftY,box3DGroundLength,box3DGroundWidth,box3DGroundCenterX,box3DGroundCenterXSigma,box3DGroundCenterY,box3DGroundCenterYSigma,box3DClosestPointX,box3DClosestPointY,box3DOrientationAngle,box3DOrientationAngleSigma,box3DHeight,box3DRelVelocityX,box3DRelVelocityXSigma,box3DRelVelocityY,box3DRelVelocityYSigma,box3DDataSource,box3DLidarInterpolationAge,box3DClassificationQuality,lidarDistanceX,lidarDistanceY,lidarVelocityX,lidarVelocityY,isInvalid,isStatic,ObjectId,Ibeo2MarkingsVersion,IdcOdExtractorVersion,clusterID,faceVisible,leftBorderVisibility,rightBorderVisibility"
scala> val columns2 = columns.split(",")
scala> val columns3 = sc.parallelize(columns2)
scala> columns3.first()
scala> val x = sc.textFile("/sampledata/After_Markings_SMPC522A1DC0040_SE-PGA454_20171012_082108_FileInfo.txt")
scala> val pedes = x.filter(line => line.contains("pedestrian"))
scala> pedes.saveAsTextFile("file:///home/hdu/outp6")
scala> val df = spark.read.option("inferSchema","true").csv("file:///home/hdu/outp6/part-00000")
scala> val df1 = df.toDF(columns.split(","): _*)
scala> df1.select("frameNumber","streamName","rightBorderVisibility").show(5)

for related examples refer:
/Bigdata_Spark/Spark_n_Scala/SparkApplicationBasicExamples/ForCluster/MySparkApps
DfourthApp.scala
EfifthApp.scala
FsixthApp.scala
GseventhApp.scala
HeighthApp.scala
-------------
#Reading and writing different files, different formats
Ex1:
--using file from hdfs and using SparkSession which is available as 'spark' ie dataframe api (schemaRDD)
>>> val x = spark.read.load("/sampledata/Bank_full.csv")
--will fail as data is not in parquet

>>> val x = spark.read.load("/sampledata/users.parquet")
>>> x.first()

>>> val x = spark.read.format("csv").load("/sampledata/Bank_full.csv")

>>> x
res0: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 16 more fields]

>>> x.printSchema
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)
 |-- _c9: string (nullable = true)
 |-- _c10: string (nullable = true)
 |-- _c11: string (nullable = true)
 |-- _c12: string (nullable = true)
 |-- _c13: string (nullable = true)
 |-- _c14: string (nullable = true)
 |-- _c15: string (nullable = true)
 |-- _c16: string (nullable = true)
 |-- _c17: string (nullable = true)

>>>val x = spark.read.format("csv").option("header","true").option("delimiter",",").option("inferSchema","true").load("/sampledata/Bank_full.csv")

>>> x.printSchema
 |-- serNo: integer (nullable = true)
 |-- age: integer (nullable = true)
 |-- job: string (nullable = true)
 |-- marital: string (nullable = true)
 |-- education: string (nullable = true)
 |-- defaulter: string (nullable = true)
 |-- balance: integer (nullable = true)
 |-- housing: string (nullable = true)
 |-- loan: string (nullable = true)
 |-- contact: string (nullable = true)
 |-- day: integer (nullable = true)
 |-- month: string (nullable = true)
 |-- duration: integer (nullable = true)
 |-- campaign: integer (nullable = true)
 |-- pdays: integer (nullable = true)
 |-- previous: integer (nullable = true)
 |-- poutcome: string (nullable = true)
 |-- y: string (nullable = true)

x.filter(x("marital") === "married").show(10)
x.filter(x("marital") === "married").count()

Ex2:
Option1:
option 2: directly reading json file
>>> val df = spark.read.json("/sampledata/people.json")

>>> df.show()

>>> df.printSchema()

>>> df.select("age").show()

>>> df.select("age","address").show()

>>> df.select(df("age")).show()

>>> df.select(df("age") > 30).show()

>>> df.filter(df("age") > 30).show()

>>> df.groupBy("age").count().show()

>>>df.groupBy("age").count().write.saveAsTable("dbmyfirstdf")
or
>>> df.groupBy("age").count().write.mode('overwrite').saveAsTable("dbmyfirstdf")
--to be checked

--using hive if database exists
>>>spark.sql("use ajmydb")

or
>>> val x = spark.sql("select * from dbmyfirstdf limit 10")
x = spark.sql("select * from dbmyfirstdf limit 10").show(2)

Option2:
option 1.1: defining schema and reading json file

import org.apache.spark.sql.Row
import org.apache.spark.sql.types._

val schema = new StructType().
      add("id", IntegerType, true).
      add("name", StringType, true).
      add("age", IntegerType, true).
      add("city", StringType, true).
      add("address", StringType, true)

val people = spark.read.schema(myschema).json("/sampledata/people.json")
people.show()

[--if using a multiline json file
val recipes = spark.read.option("multiline","true").json("/sampledata/recipes-orig.json")


#writes data in parquet
>>> val df = spark.read.json("/sampledata/people.json")
>>> df.groupBy("age").count().write.mode("overwrite").save("dbmyfirstdf")

#write the data in json
>>> df.groupBy("age").count().write.format("json").mode("overwrite").save("dbmyfirstdf")
>>> df.groupBy("age").count().write.format("csv").mode("overwrite").save("dbmyfirstdf1")
>>> df.groupBy("age").count().write.format("csv").mode("append").save("dbmyfirstdf1")

>>> df.createOrReplaceTempView("people")
>>> spark.sql("select * from people")

>>> df.createGlobalTempView("people1")
>>> spark.sql("select * from global_temp.people1").show()
>>> spark.newSession().sql("select * from global_temp.people1").show()

write the df in different formats
>>> df.rdd.getNumPartitions() 
>>> df.write.format("json").mode("overwrite").save("dbmyfirstdf2")
>>> df2 = spark.read.json("dbmyfirstdf2/part-00000-0b8ce26e-ddfe-4388-a16a-d652edd66d97-c000.json")                                     
#creating df from xml
spark-shell --packages com.databricks:spark-xml_2.11:0.12.0 --master local
scala> val df = spark.read.format("com.databricks.spark.xml").option("rowTag", "book").load("/sampledata/books.xml")
OR
scala> val df1 = spark.read.format("xml").option("rowTag", "book").load("/sampledata/books.xml")
scala> df.show()
scala> df.show(truncate=false)

#specify multiple packages to work with xml and avro
spark-shell --packages com.databricks:spark-xml_2.11:0.12.0,org.apache.spark:spark-avro_2.11:2.4.3 --master local
scala> val df = spark.read.format("xml").option("rowTag", "book").load("/sampledata/books.xml")

scala> df.write.format("avro").save("/sampledata/books")

scala> df.write.format("avro").mode("overwrite").save("/sampledata/books")

scala> val df1 = spark.read.format("avro").load("/sampledata/books/part-00000-ea8b5649-52e4-4359-a396-94a43e7e28f4-c000.avro")

scala> df1.show()

#Spark DataFrameWriter provides partitionBy() function to partition the Avro at the time of writing. Partition improves performance on reading by reducing Disk I/O. 
scala> val data = Seq(("James ","","Smith",2018,1,"M",3000),
     |       ("Michael ","Rose","",2010,3,"M",4000),
     |       ("Robert ","","Williams",2010,3,"M",4000),
     |       ("Maria ","Anne","Jones",2005,5,"F",4000),
     |       ("Jen","Mary","Brown",2010,7,"",-1)
     |     )

scala> val columns = Seq("firstname", "middlename", "lastname", "dob_year",
     |  "dob_month", "gender", "salary")

scala> import spark.sqlContext.implicits._
import spark.sqlContext.implicits._

scala> val df = data.toDF(columns:_*)

scala> df.write.partitionBy("dob_year","dob_month")
res3: org.apache.spark.sql.DataFrameWriter[org.apache.spark.sql.Row] = org.apache.spark.sql.DataFrameWriter@1914bfa1

scala> df.write.partitionBy("dob_year","dob_month").format("avro").save("people")
                                                                                
scala> df.write.partitionBy("dob_year","dob_month").format("avro").save("/sampledata/people")

scala> spark.read.format("avro").load("/sampledata/people").where(col("dob_year") === 2010).show()

scala> spark.sql("CREATE TEMPORARY VIEW PERSON USING avro options (path \"/sampledata/people\")") 

scala> spark.sql("SELECT * FROM PERSON").show()

--using avro schemas
Avro schemas are usually defined with .avsc extension and the format of the file is in JSON. We will store below schema in person.avsc file and provide this file using option() while reading an Avro file. This schema provides the structure of the Avro file with field names and it’s data types.


--
vi person.avsc
{
  "type": "record",
  "name": "Person",
  "namespace": "com.sparkbyexamples",
  "fields": [
    {"name": "firstname","type": "string"},
    {"name": "middlename","type": "string"},
    {"name": "lastname","type": "string"},
    {"name": "dob_year","type": "int"},
    {"name": "dob_month","type": "int"},
    {"name": "gender","type": "string"},
    {"name": "salary","type": "int"}
  ]
}



>>>val df = spark.read.format("csv").option("header","true").option("delimiter",",").option("inferSchema","true").load("/sampledata/Bank_full.csv")
>>> df2.show(2)

>>> df.filter(df("balance") > 2000).count()
>>> df.filter(df("balance") > 2000 and (df("marital") === "married")).count()
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).count() 
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).show(10)
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).sort(df("balance")).show(10) 
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).sort(df("balance")).groupBy(df("job")).count().show()
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).sort(df("balance")).groupBy(df("job")).count().sort("count").show()
>>> df.filter(df("balance") > 2000 and (df("marital") === "married") and (df("education") === "tertiary")).sort(df("balance")).groupBy(df("job")).count().sort(desc("count")).show()

scala> bankDF.select(bankDF("marital")).distinct.show()

scala> val subscCount = bankDF.filter(bankDF("y") === "yes").count().toDouble
subscCount: Double = 5289.0

scala> val totCount = bankDF.count().toDouble
totCount: Double = 45211.0

scala> val sucRate = subscCount/totCount
sucRate: Double = 0.11698480458295547

scala> val sucRate = subscCount/totCount * 100
sucRate: Double = 11.698480458295547

scala> val falRate = 100 - sucRate
falRate: Double = 88.30151954170445

scala> bankDF.groupBy(bankDF("marital"),bankDF("age"),bankDF("y")).count().filter((col("count") > 500) and (col("marital") === "married")).show()


--task(sort the above group by result in descending order of count and find top 10 counts)

scala> bankDF.groupBy(bankDF("marital"),bankDF("age"),bankDF("y")).count().filter((col("count") > 500) and (col("marital") === "married")).sort(desc("marital")).show()

scala> bankDF.groupBy(bankDF("marital"),bankDF("age"),bankDF("y")).count().filter((col("count") > 500) and (col("marital") === "married")).
write.option("header","true").
mode("overwrite").
csv("/sampledata/myfirstdf4")

scala> val df = bankDF.groupBy(bankDF("marital"),bankDF("age"),bankDF("y")).count().filter((col("count") > 500) and (col("marital") === "married")).sort(desc("marital"))
scala> df.rdd.getNumPartitions()

>>> df.
write.option("header","true").
partitionBy("marital").
mode("overwrite").
csv("/sampledata/myfirstdf5")


scala> newdf = spark.read.option("inferSchema","true").option("header","true").csv("/sampledata/myfirstdf5/marital=married")


scala> bankDF.groupBy(bankDF("marital"),bankDF("age"),bankDF("y")).count().filter((col("count") > 500) and (col("marital") === "married")). 
repartition(1).
write.option("header","true").
partitionBy("marital").
mode("overwrite").
csv("/sampledata/myfirstdf7")

-------------------------------------------
write the df with overwrite n append options
write the df in a hive table

================
#Dataset example

scala> val x = spark.read.text("/sampledata/abc1.txt").as[String]
x: org.apache.spark.sql.Dataset[String] = [value: string]

scala> x
res1: org.apache.spark.sql.Dataset[String] = [value: string]

scala> x.flatMap(n => n.split(" ")).groupByKey(_.toLowerCase).count
res5: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]

scala> x.flatMap(n => n.split(" ")).groupByKey(_.toLowerCase).count.show()

------------
#Using reflective approach
scala> case class Person(name: String, age: Int)
defined class Person

scala> val peopleDF = sc.textFile("/sampledata/people.txt").map(_.split(",")).map(p => Person(p(0),p(1).trim.toInt))
peopleDF: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[14] at map at <console>:26

scala> val peopleDF1 = peopleDF.toDF()
peopleDF1: org.apache.spark.sql.DataFrame = [name: string, age: int]

scala> peopleDF1.createOrReplaceTempView("people")

scala> teenagersDF.map(t => "Name:" + t(0)).collect().foreach(println)

scala> teenagersDF.map(t => "Name:" + t.getAs[String]("name")).collect().foreach(println)

scala> teenagersDF.map(_.getValuesMap[Any](List("Name","age"))).collect().foreach(println)

#Using Programmatic approach
val people = sc.textFile("/sampledata/people.txt")
val schemaString = "name age"
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{StructType, StructField, StringType}
val schema = StructType( schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, true)))
val rowRDD = people.map(_.split(",")).map(p => Row(p(0), p(1).trim))
val peopleDataFrame = spark.createDataFrame(rowRDD, schema)
peopleDataFrame.createOrReplaceTempView("people")
spark.sql("select * from people").show()

---------
#schema is description of the structure of data 
val schemaUntyped = new StructType().add("name", "String").add("age", "Int")

#Describe schema of strongly typed datasets using Encoders
import org.apache.spark.sql.Encoders
case class Person(id: Long, name: String)
Encoders.product[Person].schema

#
--creating emp and dept DF

  val emp = Seq((1,"Smith",-1,"2018","10","M",3000),
    (2,"Rose",1,"2010","20","M",4000),
    (3,"Williams",1,"2010","10","M",1000),
    (4,"Jones",2,"2005","10","F",2000),
    (5,"Brown",2,"2010","40","",-1),
      (6,"Brown",2,"2010","50","",-1)
  )
  val empColumns = Seq("emp_id","name","superior_emp_id","year_joined",
       "emp_dept_id","gender","salary")
  import spark.sqlContext.implicits._
  val empDF = emp.toDF(empColumns:_*)
  empDF.show(false)

  val dept = Seq(("Finance",10),
    ("Marketing",20),
    ("Sales",30),
    ("IT",40)
  )

  val deptColumns = Seq("dept_name","dept_id")
  val deptDF = dept.toDF(deptColumns:_*)
  deptDF.show(false)

#Inner Join
Inner join is the default join and commonly  used, It is used to join two DataFrames/Datasets on key columns, and where keys don’t match the rows get dropped from both datasets (emp & dept).

empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"inner")
    .show(false)

#Full Outer Join
Outer i.e. full, fullouter join returns all rows from both Spark DataFrame/Datasets, where join expression doesn’t match it returns null on respective record columns.

  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"outer")
    .show(false)
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"full")
    .show(false)
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"fullouter")
    .show(false)

#Left Outer Join
Spark Left i.e. Left Outer join returns all rows from the left DataFrame/Dataset regardless of match found on the right dataset when join expression doesn’t match, it assigns null for that record and drops records from right where match not found.


  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"left")
    .show(false)
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"leftouter")
    .show(false)

#Right outer join
Spark Right i.e.  Right Outer join is opposite of left join, here it returns all rows from the right DataFrame/Dataset regardless of math found on the left dataset, when join expression doesn’t match, it assigns null for that record and drops records from left where match not found.

  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"right")
   .show(false)
  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"rightouter")
   .show(false)

#Left semi join

Left Semi join is similar to inner join difference being leftsemi join returns all columns from the left DataFrame/Dataset and ignores all columns from the right dataset. In other words, this join returns columns from the only left dataset for the records match in the right dataset on join expression, records not matched on join expression are ignored from both left and right datasets.


  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"leftsemi")
    .show(false)

#Left Anti join
Left Anti join does the exact opposite of the Spark leftsemi join, leftanti join returns only columns from the left DataFrame/Dataset for non-matched records.

  empDF.join(deptDF,empDF("emp_dept_id") ===  deptDF("dept_id"),"leftanti")
    .show(false)

#Self Join
Spark Joins are not complete without a self join, Though there is no self-join type available, we can use any of the above-explained join types to join DataFrame to itself. 

  empDF.as("emp1").join(empDF.as("emp2"),
    col("emp1.superior_emp_id") === col("emp2.emp_id"),"inner")
    .select(col("emp1.emp_id"),col("emp1.name"),
      col("emp2.emp_id").as("superior_emp_id"),
      col("emp2.name").as("superior_emp_name"))
      .show(false)


#Using SQL Expression

  empDF.createOrReplaceTempView("EMP")
  deptDF.createOrReplaceTempView("DEPT")
//SQL JOIN
  val joinDF = spark.sql("select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id")
  joinDF.show(false)

  val joinDF2 = spark.sql("select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id")
  joinDF2.show(false)


